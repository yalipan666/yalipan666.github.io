---
layout: archive
title: ""
permalink: /research/
author_profile: true
---

<style>
.research-section {
    display: flex;
    align-items: center;
    gap: 30px;
    margin-bottom: 50px;
    padding: 20px;
    background: transparent;
    border-radius: 10px;
    border: 1px solid rgba(128, 128, 128, 0.2);
}

.research-section img {
    width: 300px;
    height: 300px;
    object-fit: cover;
    border-radius: 10px;
    flex-shrink: 0;
}

.research-section .text {
    flex: 1;
}

.research-section h3 {
    margin-top: 0;
    margin-bottom: 15px;
    font-size: 22px;  /* Heading size */
}
    
.research-section p {
    font-size: 0.9em;  /* 90% of default size */
    line-height: 1.15;  /* Space between lines */
}
   

/* Mobile responsive */
@media (max-width: 768px) {
    .research-section {
        flex-direction: column;
    }
    
    .research-section img {
        width: 100%;
        height: auto;
    }
      .research-section h3 {
        font-size: 20px;  /* Smaller heading on mobile */
    }
    
    .research-section p {
        font-size: 15px;  /* Smaller text on mobile */
    }
}
</style>

<!-- Section 1 -->
<div class="research-section">
    <img src="/images/research1.jpg" alt="Research Project 1">
    <div class="text">
        <h3>Measuring Attention in "the Wild"</h3>
        <p>Traditional attention studies restricted eye movements, treating them as artefacts, yet growing evidence shows they carry cognitive significance. The challenge then becomes: how can we measure attention in the presence of natural eye movement? Rapid Invisible Frequency Tagging (RIFT) uses high-frequency flicker (≥60 Hz) that is imperceptible to participants yet evokes rhythmic activity in the visual cortex. By “tagging” specific items, RIFT allows us to track attention to multiple items simultaneously, including both foveal and parafoveal regions (i.e., even before fixating on the item). This capability opens the door to studying attention in "the wild", i.e., more naturalistic and ecological contexts, where eye movements are not treated as noise but as an integral part of cognitive processing.</p>
    </div>
</div>

<!-- Section 2 -->
<div class="research-section">
    <img src="/images/research2.jpg" alt="Research Project 2">
    <div class="text">
        <h3>"Looking Ahead" in Reading</h3>
        <p>Skilled readers can extract information from a word even before directly fixating on it, but how much can actually be gathered from the parafoveal region? To address this, we co-register eye tracking with MEG (and OPM-MEG for studying reading development in children). We provide neural evidence that substantial information can be pre-processed before fixation, a result not always reflected in eye movement data, underscoring the importance of integrating neural evidence when building theories of reading</p>
    </div>
</div>

<!-- Section 3 -->
<div class="research-section">
    <img src="/images/research3.jpg" alt="Research Project 3">
    <div class="text">
        <h3>"Read the Mind"</h3>
        <p>.</p>
    </div>
</div>

<!-- Section 4 -->
<div class="research-section">
    <img src="/images/research4.jpg" alt="Research Project 4">
    <div class="text">
        <h3>Brain-Inspired AI for Language Processing</h3>
        <p>I am exploring whether knowledge from neuroscience can help build smarter AI models. By incorporating principles of how the human brain processes language, we aim to develop more efficient and interpretable neural networks. This interdisciplinary approach bridges cognitive neuroscience and machine learning.</p>
    </div>
</div>




